{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_names = []\n",
    "path_df = '../datasets/original_files/'\n",
    "for (dirpath, dirnames, filenames) in walk(path_df):\n",
    "    datasets_names.extend(filenames)\n",
    "    break\n",
    "datasets_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataFrame = False\n",
    "first = True\n",
    "for name in datasets_names:\n",
    "    if(first):\n",
    "        first = False\n",
    "        dataFrame = pd.read_csv(path_df+name, compression='gzip')\n",
    "        dataFrame.set_index('id', drop=False, inplace=True)\n",
    "        print('adding', len(dataFrame), 'rows')\n",
    "    else:\n",
    "        _tmpDf = pd.read_csv(path_df+name, compression='gzip')\n",
    "        _tmpDf.set_index('id', drop=False, inplace=True)\n",
    "        print('adding', len(_tmpDf), 'rows')\n",
    "        dataFrame = pd.concat([dataFrame, _tmpDf])\n",
    "        del _tmpDf\n",
    "print('total rows:', len(dataFrame))\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cleaning columns\n",
    "for x in dataFrame.columns:\n",
    "    print(x)\n",
    "    values = dataFrame[x][pd.notna(dataFrame[x])].values\n",
    "    if(len(values) > 0):\n",
    "        print(values[0])\n",
    "    else:\n",
    "        print('NOT_VALUES')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neighbourhood\n",
    "# neighbourhood_cleansed\n",
    "# neighbourhood_group_cleansed\n",
    "# guests_included\n",
    "# license\n",
    "# is_business_travel_ready\n",
    "\n",
    "cols_to_drop = [\n",
    "    'market', 'street',\n",
    "    'listing_url', 'scrape_id', 'thumbnail_url', 'medium_url', 'picture_url', 'xl_picture_url', 'host_id', 'host_url',\n",
    "    'host_name', 'host_thumbnail_url', 'host_picture_url', 'host_neighbourhood',\n",
    "    'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'zipcode', 'smart_location', 'country_code',\n",
    "    'weekly_price', 'monthly_price', 'security_deposit', 'cleaning_fee', 'extra_people',\n",
    "    'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights',\n",
    "    'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'calendar_updated', 'has_availability',\n",
    "    'requires_license','jurisdiction_names','host_location'\n",
    "]\n",
    "# [colName for colName in dataFrame.columns if colName not in cols_to_drop]\n",
    "\n",
    "\n",
    "dataFrame.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text pipeline steps\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "#Esta clase simplemente filtra las columnas que se le indica en el constructor\n",
    "class FeatureSelector( BaseEstimator, TransformerMixin ):\n",
    "    #Class Constructor \n",
    "    def __init__( self, feature_names ):\n",
    "        self._feature_names = feature_names \n",
    "    \n",
    "    #Return self nothing else to do here    \n",
    "    def fit( self, X, y = None ):\n",
    "        return self \n",
    "    \n",
    "    #Method that describes what we need this transformer to do\n",
    "    def transform( self, X, y = None ):\n",
    "        return X[ self._feature_names ]\n",
    "    \n",
    "class TextTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __clean_text(self, x):\n",
    "        for punct in \"/-'\":\n",
    "            x = x.replace(punct, ' ')\n",
    "        for punct in '&':\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "        for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~•' + '“”’':\n",
    "            x = x.replace(punct, '')\n",
    "        return x.lower()\n",
    "    \n",
    "    def __parseTextCols(self, x):\n",
    "        finalTexts = []\n",
    "        for i in x:\n",
    "            if(pd.notna(i) and i not in finalTexts):\n",
    "                finalTexts.append(i)\n",
    "        text = self.__clean_text(' '.join(finalTexts))\n",
    "        return text\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform (self, X, y = None):\n",
    "        return X.apply(self.__parseTextCols, axis=1)\n",
    "    \n",
    "    \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class custom_Tfidf(TfidfVectorizer, TransformerMixin):\n",
    "    options= {\n",
    "        'fitSample': 1\n",
    "    }\n",
    "    def __init__(self, params, options = None):\n",
    "        self.vectorizer = TfidfVectorizer(**params)\n",
    "        if(options != None):\n",
    "            for key in options.keys():\n",
    "                self.options[key] = options[key]\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        self.vectorizer.fit(X.sample(frac=self.options['fitSample']))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        print(len(X))\n",
    "        return self.vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "text_cols = [\"name\",\"summary\",\"space\",\"description\",\"neighborhood_overview\",\"notes\",\"transit\",\"access\",\n",
    "             \"interaction\",\"house_rules\",\"host_about\"]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words('english')\n",
    "textVectSettings = {\n",
    "    'stop_words': stopwords_en,\n",
    "    'max_df': 0.90,\n",
    "    'min_df': .05,\n",
    "    'ngram_range': (1,3),\n",
    "    'max_features': 350\n",
    "}\n",
    "\n",
    "#Pasos para el pipeline Textos\n",
    "text_pipeline = Pipeline(steps = [\n",
    "    ( 'text_selector', FeatureSelector(text_cols) ),\n",
    "    ( 'text_transformer', TextTransformer() ),\n",
    "    ( 'text_vectorize',  custom_Tfidf(textVectSettings, {'fitSample':.01}))\n",
    "] )\n",
    "\n",
    "text_pipeline.fit(dataFrame)\n",
    "# text_pipeline.transform(dataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_cols = [\"instant_bookable\",\"is_business_travel_ready\",\"cancellation_policy\",\n",
    "\"require_guest_phone_verification\",\n",
    "\"require_guest_profile_picture\",\"host_response_time\",\n",
    "\"host_is_superhost\",\"host_has_profile_pic\",\"host_identity_verified\",\n",
    "\"city\",\"state\",\"property_type\",\"room_type\",\"bed_type\"];\n",
    "\n",
    "dummy_pipeline = Pipeline(steps = [\n",
    "    ('dummy_selector', FeatureSelector(dummy_cols))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "class customTransformer:\n",
    "    \n",
    "    textCols = []\n",
    "    configuration = {\n",
    "        'textTrainSample': .3\n",
    "    }\n",
    "    \n",
    "    def __init__(self, columns, configuration = False):\n",
    "        self.fitted = False\n",
    "        from nltk.corpus import stopwords\n",
    "        from sklearn.feature_extraction.text import TfidfTransformer\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        stopwords_en = stopwords.words('english')\n",
    "        \n",
    "        textVectSettings = {\n",
    "            'stop_words': stopwords_en,\n",
    "            'max_df': 0.90,\n",
    "            'min_df': .05,\n",
    "            'ngram_range': (1,3),\n",
    "            'max_features': 350\n",
    "        }\n",
    "        \n",
    "        if(configuration != False):\n",
    "            for key in configuration.keys():\n",
    "                self.configuration[key] = configuration[key]\n",
    "        self.Tfid = TfidfVectorizer(**textVectSettings)\n",
    "        self.textCols = columns['textCols']\n",
    "        \n",
    "    def __clean_text(self, x):\n",
    "        for punct in \"/-'\":\n",
    "            x = x.replace(punct, ' ')\n",
    "        for punct in '&':\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "        for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~•' + '“”’':\n",
    "            x = x.replace(punct, '')\n",
    "        return x.lower()\n",
    "    \n",
    "    def __parseTextCols(self, x):\n",
    "        finalTexts = []\n",
    "        for x in x[self.textCols]:\n",
    "            if(pd.notna(x) and x not in finalTexts):\n",
    "                finalTexts.append(x)\n",
    "        text = self.__clean_text(' '.join(finalTexts))\n",
    "        return text\n",
    "    \n",
    "    def fit_transform(self, x):\n",
    "        self.fit(x)\n",
    "        return self.transform(x)\n",
    "    \n",
    "    def fit(self, x):\n",
    "        \"\"\"\n",
    "            Learn parameters from pandas dataframe.\n",
    "        \"\"\"\n",
    "        sample = self.configuration['textTrainSample']\n",
    "        texts = x.sample(frac=sample).apply(self.__parseTextCols, axis=1)\n",
    "        self.Tfid.fit(texts)\n",
    "        self.fitted= True\n",
    "        \n",
    "    def transform(self, x):\n",
    "        \"\"\"\n",
    "            Transform pandas dataframe to matrix.\n",
    "            ----\n",
    "            Retruns matrix.\n",
    "        \"\"\"\n",
    "        from scipy.sparse import hstack\n",
    "        if(not self.fitted):\n",
    "            raise NameError('customTransformer not fitted')\n",
    "        texts = x.apply(self.__parseTextCols, axis=1)\n",
    "        textVectors = self.Tfid.transform(texts)\n",
    "        return hstack([textVectors])\n",
    "    \n",
    "    def save_pkl(slef, filename):\n",
    "        \"\"\"\n",
    "            Save class with parameters to a .pkl file.\n",
    "        \"\"\"\n",
    "        with open(filename, 'wb') as output:\n",
    "            pickle.dump(self, output, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "        \n",
    "cols = {\n",
    "    'textCols': [\"name\",\"summary\",\"space\",\"description\",\"neighborhood_overview\",\"notes\",\"transit\",\"access\",\n",
    "             \"interaction\",\"house_rules\",\"host_about\"]\n",
    "}\n",
    "configuration = {\n",
    "    'textTrainSample': .01\n",
    "}\n",
    "transformer = customTransformer(cols, configuration)\n",
    "transformer.fit(dataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
